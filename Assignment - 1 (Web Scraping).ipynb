{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4e57590",
   "metadata": {},
   "source": [
    "                                     ## ASSIGNMENT-1\n",
    "                                    ### WEB SCRAPING\n",
    "In all the following questions, you have to use BeautifulSoup to scrape different websites and collect data as per\n",
    "the requirement of the question.\n",
    "Every answer to the question should be in form of a python function which should take URL as the parameter.\n",
    "Use Jupyter Notebooks to program, upload it on your GitHub and send the link of the Jupyter notebook to\n",
    "your SME."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c58ec771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all the header tags :\n",
      "\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>\n",
      "\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n",
      "\n",
      "<h2>Navigation menu</h2>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-personal-label\">\n",
      "<span class=\"vector-menu-heading-label\">Personal tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n",
      "<span class=\"vector-menu-heading-label\">Namespaces</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-views-label\">\n",
      "<span class=\"vector-menu-heading-label\">Views</span>\n",
      "</h3>\n",
      "\n",
      "<h3>\n",
      "<label for=\"searchInput\">Search</label>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-navigation-label\">\n",
      "<span class=\"vector-menu-heading-label\">Navigation</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-interaction-label\">\n",
      "<span class=\"vector-menu-heading-label\">Contribute</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-tb-label\">\n",
      "<span class=\"vector-menu-heading-label\">Tools</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n",
      "<span class=\"vector-menu-heading-label\">Print/export</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-wikibase-otherprojects-label\">\n",
      "<span class=\"vector-menu-heading-label\">In other projects</span>\n",
      "</h3>\n",
      "\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-lang-label\">\n",
      "<span class=\"vector-menu-heading-label\">Languages</span>\n",
      "</h3>\n"
     ]
    }
   ],
   "source": [
    "# 1) Write a python program to display all the header tags from wikipedia.org.\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "print('List all the header tags :', *titles, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e49e8f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name of Movie</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.234908068548295</td>\n",
       "      <td>1994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.156212618407485</td>\n",
       "      <td>1972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>8.98869097277759</td>\n",
       "      <td>2008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>8.984180062381316</td>\n",
       "      <td>1974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>8.950380690167343</td>\n",
       "      <td>1957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Lawrence of Arabia</td>\n",
       "      <td>8.25394420868137</td>\n",
       "      <td>1962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>M - Eine Stadt sucht einen Mörder</td>\n",
       "      <td>8.253266899208677</td>\n",
       "      <td>1931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Idi i smotri</td>\n",
       "      <td>8.251719496864853</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>North by Northwest</td>\n",
       "      <td>8.2496548388162</td>\n",
       "      <td>1959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>10</td>\n",
       "      <td>Vertigo</td>\n",
       "      <td>8.246038504257763</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                      Name of Movie            Ratings  Year\n",
       "0     1           The Shawshank Redemption  9.234908068548295  1994\n",
       "1     2                      The Godfather  9.156212618407485  1972\n",
       "2     3                    The Dark Knight   8.98869097277759  2008\n",
       "3     4              The Godfather Part II  8.984180062381316  1974\n",
       "4     5                       12 Angry Men  8.950380690167343  1957\n",
       "..  ...                                ...                ...   ...\n",
       "95   96                 Lawrence of Arabia   8.25394420868137  1962\n",
       "96   97  M - Eine Stadt sucht einen Mörder  8.253266899208677  1931\n",
       "97   98                       Idi i smotri  8.251719496864853  1985\n",
       "98   99                 North by Northwest    8.2496548388162  1959\n",
       "99   10                            Vertigo  8.246038504257763  1958\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release) and make data frame.\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    "    \n",
    "Page = requests.get ('http://www.imdb.com/chart/top')\n",
    "\n",
    "\n",
    "\n",
    "soup = BeautifulSoup(Page.text, \"html.parser\")\n",
    "          \n",
    "movies = soup.select('td.titleColumn')\n",
    "\n",
    "ratings = [b.attrs.get('data-value')\n",
    "        for b in soup.select('td.posterColumn span[name=ir]')]\n",
    "\n",
    "\n",
    "list = []\n",
    " \n",
    "\n",
    "for movie in list:\n",
    "    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n",
    "        ') -', movie['rating'])\n",
    "\n",
    "    \n",
    "df = pd.DataFrame(list)\n",
    "\n",
    "for index in range(0, len(movies)):\n",
    "     \n",
    "\n",
    "    movie_string = movies[index].get_text()\n",
    "    movie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "    movie_title = movie[len(str(index))+1:-7]\n",
    "    year = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "    place = movie[:len(str(index))-(len(movie))]\n",
    "    data = {\"Rank\": place,\n",
    "            \"Name of Movie\": movie_title,\n",
    "            \"Ratings\": ratings[index],\n",
    "            \"Year\": year,}\n",
    "            \n",
    "    list.append(data)\n",
    "    \n",
    "\n",
    "\n",
    "df = pd.DataFrame(list)\n",
    "\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd9f0d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name of Movie</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>Year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Kantara</td>\n",
       "      <td>8.541837070637396</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Ramayana: The Legend of Prince Rama</td>\n",
       "      <td>8.496214736122745</td>\n",
       "      <td>1993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Rocketry: The Nambi Effect</td>\n",
       "      <td>8.436331681265633</td>\n",
       "      <td>2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Nayakan</td>\n",
       "      <td>8.398451283444816</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Anbe Sivam</td>\n",
       "      <td>8.398258016827713</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96</td>\n",
       "      <td>Ustad Hotel</td>\n",
       "      <td>8.021902519848547</td>\n",
       "      <td>2012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>Theeran Adhigaaram Ondru</td>\n",
       "      <td>8.02019294891196</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>Rang De Basanti</td>\n",
       "      <td>8.018232400141432</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99</td>\n",
       "      <td>Baahubali 2: The Conclusion</td>\n",
       "      <td>8.016491994200805</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>10</td>\n",
       "      <td>Angoor</td>\n",
       "      <td>8.014613075143812</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rank                        Name of Movie            Ratings  Year\n",
       "0     1                              Kantara  8.541837070637396  2022\n",
       "1     2  Ramayana: The Legend of Prince Rama  8.496214736122745  1993\n",
       "2     3           Rocketry: The Nambi Effect  8.436331681265633  2022\n",
       "3     4                              Nayakan  8.398451283444816  1987\n",
       "4     5                           Anbe Sivam  8.398258016827713  2003\n",
       "..  ...                                  ...                ...   ...\n",
       "95   96                          Ustad Hotel  8.021902519848547  2012\n",
       "96   97             Theeran Adhigaaram Ondru   8.02019294891196  2017\n",
       "97   98                      Rang De Basanti  8.018232400141432  2006\n",
       "98   99          Baahubali 2: The Conclusion  8.016491994200805  2017\n",
       "99   10                               Angoor  8.014613075143812  1982\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of release) and make data frame.\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import pandas as pd\n",
    " \n",
    "    \n",
    "Page = requests.get ('https://www.imdb.com/india/top-rated-indian-movies/')\n",
    "\n",
    "soup = BeautifulSoup(Page.text, \"html.parser\")\n",
    "          \n",
    "movies = soup.select('td.titleColumn')\n",
    "\n",
    "ratings = [b.attrs.get('data-value')\n",
    "        for b in soup.select('td.posterColumn span[name=ir]')]\n",
    "\n",
    "\n",
    "list = []\n",
    " \n",
    "\n",
    "for movie in list:\n",
    "    print(movie['place'], '-', movie['movie_title'], '('+movie['year'] +\n",
    "        ') -', movie['rating'])\n",
    "\n",
    "    \n",
    "df = pd.DataFrame(list)\n",
    "\n",
    "for index in range(0, len(movies)):\n",
    "     \n",
    "\n",
    "    movie_string = movies[index].get_text()\n",
    "    movie = (' '.join(movie_string.split()).replace('.', ''))\n",
    "    movie_title = movie[len(str(index))+1:-7]\n",
    "    year = re.search('\\((.*?)\\)', movie_string).group(1)\n",
    "    place = movie[:len(str(index))-(len(movie))]\n",
    "    data = {\"Rank\": place,\n",
    "            \"Name of Movie\": movie_title,\n",
    "            \"Ratings\": ratings[index],\n",
    "            \"Year\": year,}\n",
    "            \n",
    "    list.append(data)\n",
    "    \n",
    "\n",
    "\n",
    "df = pd.DataFrame(list)\n",
    "\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0decc73d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of President</th>\n",
       "      <th>Term of Office</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Shri Ram Nath Kovind (birth - 1945)</td>\n",
       "      <td>25 July, 2017 to 25 July, 2022 \\nhttps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shri Pranab Mukherjee (1935-2020)</td>\n",
       "      <td>25 July, 2012 to 25 July, 2017 \\nhttp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Smt Pratibha Devisingh Patil (birth - 1934)</td>\n",
       "      <td>25 July, 2007 to 25 July, 2012 \\nhttp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DR. A.P.J. Abdul Kalam (1931-2015)</td>\n",
       "      <td>25 July, 2002 to 25 July, 2007 \\nhttp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Shri K. R. Narayanan (1920 - 2005)</td>\n",
       "      <td>25 July, 1997 to 25 July, 2002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Dr Shankar Dayal Sharma (1918-1999)</td>\n",
       "      <td>25 July, 1992 to 25 July, 1997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Shri R Venkataraman (1910-2009)</td>\n",
       "      <td>25 July, 1987 to 25 July, 1992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Giani Zail Singh (1916-1994)</td>\n",
       "      <td>25 July, 1982 to 25 July, 1987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Shri Neelam Sanjiva Reddy (1913-1996)</td>\n",
       "      <td>25 July, 1977 to 25 July, 1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Dr. Fakhruddin Ali Ahmed (1905-1977)</td>\n",
       "      <td>24 August, 1974 to 11 February, 1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Shri Varahagiri Venkata Giri (1894-1980)</td>\n",
       "      <td>3 May, 1969 to 20 July, 1969 and 24 August, 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Dr. Zakir Husain (1897-1969)</td>\n",
       "      <td>13 May, 1967 to 3 May, 1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Dr. Sarvepalli Radhakrishnan (1888-1975)</td>\n",
       "      <td>13 May, 1962 to 13 May, 1967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Dr. Rajendra Prasad (1884-1963)</td>\n",
       "      <td>26 January, 1950 to 13 May, 1962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Name of President  \\\n",
       "0           Shri Ram Nath Kovind (birth - 1945)   \n",
       "1             Shri Pranab Mukherjee (1935-2020)   \n",
       "2   Smt Pratibha Devisingh Patil (birth - 1934)   \n",
       "3            DR. A.P.J. Abdul Kalam (1931-2015)   \n",
       "4            Shri K. R. Narayanan (1920 - 2005)   \n",
       "5           Dr Shankar Dayal Sharma (1918-1999)   \n",
       "6               Shri R Venkataraman (1910-2009)   \n",
       "7                  Giani Zail Singh (1916-1994)   \n",
       "8         Shri Neelam Sanjiva Reddy (1913-1996)   \n",
       "9          Dr. Fakhruddin Ali Ahmed (1905-1977)   \n",
       "10     Shri Varahagiri Venkata Giri (1894-1980)   \n",
       "11                 Dr. Zakir Husain (1897-1969)   \n",
       "12     Dr. Sarvepalli Radhakrishnan (1888-1975)   \n",
       "13             Dr. Rajendra Prasad (1884-1963)    \n",
       "\n",
       "                                       Term of Office  \n",
       "0              25 July, 2017 to 25 July, 2022 \\nhttps  \n",
       "1               25 July, 2012 to 25 July, 2017 \\nhttp  \n",
       "2               25 July, 2007 to 25 July, 2012 \\nhttp  \n",
       "3               25 July, 2002 to 25 July, 2007 \\nhttp  \n",
       "4                      25 July, 1997 to 25 July, 2002  \n",
       "5                      25 July, 1992 to 25 July, 1997  \n",
       "6                      25 July, 1987 to 25 July, 1992  \n",
       "7                      25 July, 1982 to 25 July, 1987  \n",
       "8                      25 July, 1977 to 25 July, 1982  \n",
       "9                24 August, 1974 to 11 February, 1977  \n",
       "10  3 May, 1969 to 20 July, 1969 and 24 August, 19...  \n",
       "11                        13 May, 1967 to 3 May, 1969  \n",
       "12                       13 May, 1962 to 13 May, 1967  \n",
       "13                   26 January, 1950 to 13 May, 1962  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4) Write s python program to display list of respected former presidents of India(i.e. Name , Term of office) from https://presidentofindia.nic.in/former-presidents.htm\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import requests as R\n",
    "\n",
    "page = R.get('https://presidentofindia.nic.in/former-presidents.htm')\n",
    "\n",
    "soup = BS (page.content)\n",
    "\n",
    "Title = []\n",
    "\n",
    "for a in soup.find_all('h3'):\n",
    "    Title.append(a.text)\n",
    "\n",
    "Term = []\n",
    "\n",
    "for i in soup.find_all('div', class_ = 'presidentListing'):\n",
    "    Term.append (i.text.split(':')[1])\n",
    "Term = [item.strip() for item in Term if str(item)]\n",
    "    \n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame ({'Name of President':Title, 'Term of Office':Term})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5e76330",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Matches Played</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>England</td>\n",
       "      <td>27</td>\n",
       "      <td>3226</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>22</td>\n",
       "      <td>2,508</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>India</td>\n",
       "      <td>34</td>\n",
       "      <td>3,802</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>22</td>\n",
       "      <td>2,354</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Australia</td>\n",
       "      <td>29</td>\n",
       "      <td>3,071</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>24</td>\n",
       "      <td>2,392</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>30</td>\n",
       "      <td>2,753</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>29</td>\n",
       "      <td>2,658</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>41</td>\n",
       "      <td>2,902</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Afghanistan</td>\n",
       "      <td>18</td>\n",
       "      <td>1,238</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Country Matches Played Points Rank\n",
       "0       England             27   3226  119\n",
       "1   New Zealand             22  2,508  114\n",
       "2         India             34  3,802  112\n",
       "3      Pakistan             22  2,354  107\n",
       "4     Australia             29  3,071  106\n",
       "5  South Africa             24  2,392  100\n",
       "6    Bangladesh             30  2,753   92\n",
       "7     Sri Lanka             29  2,658   92\n",
       "8   West Indies             41  2,902   71\n",
       "9   Afghanistan             18  1,238   69"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "Country = []\n",
    "for i in soup.find_all('span',class_='u-hide-phablet'):\n",
    "    Country.append(i.text)\n",
    "\n",
    "\n",
    "Matches = []\n",
    "for i in soup.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "    Matches.append(i.text.split(',')[0])\n",
    "del Matches [1::2]\n",
    "Matches.insert (0,27)\n",
    "\n",
    "    \n",
    "Points = []\n",
    "for i in soup.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "    Points.append(i.text)\n",
    "Points\n",
    "del Points [0::2]\n",
    "Points.insert (0,3226)\n",
    "\n",
    "    \n",
    "Rating = []\n",
    "for i in soup.find_all('td',class_= 'table-body__cell u-text-right rating'):\n",
    "    Rating.append(i.text)\n",
    "Rating.insert (0,119)\n",
    "\n",
    "    \n",
    "\n",
    "df = pd.DataFrame ({'Country': Country, 'Matches Played': Matches,'Points': Points ,'Rank': Rating})\n",
    "df.head (10)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b705062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name of the Player</th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Babar Azam</td>\n",
       "      <td>PAK</td>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Imam-ul-Haq</td>\n",
       "      <td>PAK</td>\n",
       "      <td>779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Rassie van der Dussen</td>\n",
       "      <td>SA</td>\n",
       "      <td>766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Quinton de Kock</td>\n",
       "      <td>SA</td>\n",
       "      <td>759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Jonny Bairstow</td>\n",
       "      <td>ENG</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>David Warner</td>\n",
       "      <td>AUS</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Virat Kohli</td>\n",
       "      <td>IND</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Rohit Sharma</td>\n",
       "      <td>IND</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Ross Taylor</td>\n",
       "      <td>NZ</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Steve Smith</td>\n",
       "      <td>AUS</td>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank     Name of the Player Country Rating\n",
       "0    1             Babar Azam     PAK    890\n",
       "1    2            Imam-ul-Haq     PAK    779\n",
       "2    3  Rassie van der Dussen      SA    766\n",
       "3    4        Quinton de Kock      SA    759\n",
       "4    5         Jonny Bairstow     ENG    732\n",
       "5    6           David Warner     AUS    725\n",
       "6    7            Virat Kohli     IND    722\n",
       "7    8           Rohit Sharma     IND    718\n",
       "8    9            Ross Taylor      NZ    701\n",
       "9   10            Steve Smith     AUS    697"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting')\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "Rank = []\n",
    "\n",
    "for i in soup.find_all('span', class_ = 'rankings-table__pos-number'):\n",
    "    Rank.append(i.text)\n",
    "    \n",
    "Rank = [item.strip() for item in Rank if str(item)]\n",
    "Rank.insert (0,'1')\n",
    "\n",
    "Name = []\n",
    "\n",
    "for i in soup.find_all('td',class_='table-body__cell rankings-table__name name'):\n",
    "    Name.append(i.text)\n",
    "Name = [item.strip() for item in Name if str(item)]\n",
    "Name.insert (0,'Babar Azam')\n",
    "\n",
    "Country = []\n",
    "\n",
    "for i in soup.find_all('span', class_ = 'table-body__logo-text'):\n",
    "    Country.append(i.text)\n",
    "Country.insert (0, 'PAK')\n",
    "\n",
    "Rating = []\n",
    "\n",
    "for i in soup.find_all('td', class_= 'table-body__cell rating'):\n",
    "    Rating.append(i.text)\n",
    "Rating.insert (0,'890')\n",
    "Rating\n",
    "\n",
    "df = pd.DataFrame ({'Rank': Rank, 'Name of the Player': Name, 'Country': Country, 'Rating': Rating})\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84f2e26e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name of the Player</th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Trent Boult</td>\n",
       "      <td>NZ</td>\n",
       "      <td>775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Josh Hazlewood</td>\n",
       "      <td>AUS</td>\n",
       "      <td>718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Mujeeb Ur Rahman</td>\n",
       "      <td>AFG</td>\n",
       "      <td>676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Shaheen Afridi</td>\n",
       "      <td>PAK</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Mohammad Nabi</td>\n",
       "      <td>AFG</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Mehedi Hasan</td>\n",
       "      <td>BAN</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Matt Henry</td>\n",
       "      <td>NZ</td>\n",
       "      <td>654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Mitchell Starc</td>\n",
       "      <td>AUS</td>\n",
       "      <td>653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Rashid Khan</td>\n",
       "      <td>AFG</td>\n",
       "      <td>651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Jasprit Bumrah</td>\n",
       "      <td>IND</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank Name of the Player Country Rating\n",
       "0    1        Trent Boult      NZ    775\n",
       "1    2     Josh Hazlewood     AUS    718\n",
       "2    3   Mujeeb Ur Rahman     AFG    676\n",
       "3    4     Shaheen Afridi     PAK    661\n",
       "4    5      Mohammad Nabi     AFG    657\n",
       "5    6       Mehedi Hasan     BAN    655\n",
       "6    7         Matt Henry      NZ    654\n",
       "7    8     Mitchell Starc     AUS    653\n",
       "8    9        Rashid Khan     AFG    651\n",
       "9   10     Jasprit Bumrah     IND    642"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# c) Top 10 ODI bowlers along with the records of their team and rating.\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling')\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "Rank = []\n",
    "\n",
    "for i in soup.find_all('span', class_ = 'rankings-table__pos-number'):\n",
    "    Rank.append(i.text)\n",
    "    \n",
    "Rank = [item.strip() for item in Rank if str(item)]\n",
    "Rank.insert (0,'1')\n",
    "\n",
    "Name = []\n",
    "\n",
    "for i in soup.find_all('td',class_='table-body__cell rankings-table__name name'):\n",
    "    Name.append(i.text)\n",
    "Name = [item.strip() for item in Name if str(item)]\n",
    "Name.insert (0,'Trent Boult')\n",
    "\n",
    "Country = []\n",
    "\n",
    "for i in soup.find_all('span', class_ = 'table-body__logo-text'):\n",
    "    Country.append(i.text)\n",
    "Country.insert (0, 'NZ')\n",
    "\n",
    "Rating = []\n",
    "\n",
    "for i in soup.find_all('td', class_= 'table-body__cell rating'):\n",
    "    Rating.append(i.text)\n",
    "Rating.insert (0,'775')\n",
    "Rating\n",
    "\n",
    "df = pd.DataFrame ({'Rank': Rank, 'Name of the Player': Name, 'Country': Country, 'Rating': Rating})\n",
    "df.head (10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a70b5047",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Country</th>\n",
       "      <th>Matches Played</th>\n",
       "      <th>Points</th>\n",
       "      <th>Rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Australia</td>\n",
       "      <td>18</td>\n",
       "      <td>3061</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>26</td>\n",
       "      <td>3,098</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>England</td>\n",
       "      <td>25</td>\n",
       "      <td>2,904</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>India</td>\n",
       "      <td>27</td>\n",
       "      <td>2,820</td>\n",
       "      <td>104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>24</td>\n",
       "      <td>2,425</td>\n",
       "      <td>101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>24</td>\n",
       "      <td>2,334</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Bangladesh</td>\n",
       "      <td>12</td>\n",
       "      <td>932</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>21</td>\n",
       "      <td>1,237</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Ireland</td>\n",
       "      <td>11</td>\n",
       "      <td>516</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Sri Lanka</td>\n",
       "      <td>8</td>\n",
       "      <td>353</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Country Matches Played Points Rank\n",
       "0     Australia             18   3061  170\n",
       "1  South Africa             26  3,098  119\n",
       "2       England             25  2,904  116\n",
       "3         India             27  2,820  104\n",
       "4   New Zealand             24  2,425  101\n",
       "5   West Indies             24  2,334   97\n",
       "6    Bangladesh             12    932   78\n",
       "7      Pakistan             21  1,237   59\n",
       "8       Ireland             11    516   47\n",
       "9     Sri Lanka              8    353   44"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "Country = []\n",
    "for i in soup.find_all('span',class_='u-hide-phablet'):\n",
    "    Country.append(i.text)\n",
    "\n",
    "\n",
    "Matches = []\n",
    "for i in soup.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "    Matches.append(i.text.split(',')[0])\n",
    "del Matches [1::2]\n",
    "Matches.insert (0,18)\n",
    "\n",
    "    \n",
    "Points = []\n",
    "for i in soup.find_all('td',class_=\"table-body__cell u-center-text\"):\n",
    "    Points.append(i.text)\n",
    "Points\n",
    "del Points [0::2]\n",
    "Points.insert (0,3061)\n",
    "\n",
    "    \n",
    "Rating = []\n",
    "for i in soup.find_all('td',class_= 'table-body__cell u-text-right rating'):\n",
    "    Rating.append(i.text)\n",
    "Rating.insert (0,170)\n",
    "\n",
    "    \n",
    "\n",
    "df = pd.DataFrame ({'Country': Country, 'Matches Played': Matches,'Points': Points ,'Rank': Rating})\n",
    "df.head (10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d0763f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name of the Player</th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Alyssa Healy</td>\n",
       "      <td>AUS</td>\n",
       "      <td>785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Beth Mooney</td>\n",
       "      <td>AUS</td>\n",
       "      <td>749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Laura Wolvaardt</td>\n",
       "      <td>SA</td>\n",
       "      <td>732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Natalie Sciver</td>\n",
       "      <td>ENG</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Harmanpreet Kaur</td>\n",
       "      <td>IND</td>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Smriti Mandhana</td>\n",
       "      <td>IND</td>\n",
       "      <td>714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Meg Lanning</td>\n",
       "      <td>AUS</td>\n",
       "      <td>710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Rachael Haynes</td>\n",
       "      <td>AUS</td>\n",
       "      <td>701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Amy Satterthwaite</td>\n",
       "      <td>NZ</td>\n",
       "      <td>661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Chamari Athapaththu</td>\n",
       "      <td>SL</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank   Name of the Player Country Rating\n",
       "0    1         Alyssa Healy     AUS    785\n",
       "1    2          Beth Mooney     AUS    749\n",
       "2    3      Laura Wolvaardt      SA    732\n",
       "3    4       Natalie Sciver     ENG    725\n",
       "4    5     Harmanpreet Kaur     IND    716\n",
       "5    6      Smriti Mandhana     IND    714\n",
       "6    7          Meg Lanning     AUS    710\n",
       "7    8       Rachael Haynes     AUS    701\n",
       "8    9    Amy Satterthwaite      NZ    661\n",
       "9   10  Chamari Athapaththu      SL    655"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting')\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "Rank = []\n",
    "\n",
    "for i in soup.find_all('span', class_ = 'rankings-table__pos-number'):\n",
    "    Rank.append(i.text)\n",
    "    \n",
    "Rank = [item.strip() for item in Rank if str(item)]\n",
    "Rank.insert (0,'1')\n",
    "\n",
    "Name = []\n",
    "\n",
    "for i in soup.find_all('td',class_='table-body__cell rankings-table__name name'):\n",
    "    Name.append(i.text)\n",
    "Name = [item.strip() for item in Name if str(item)]\n",
    "Name.insert (0,'Alyssa Healy')\n",
    "\n",
    "Country = []\n",
    "\n",
    "for i in soup.find_all('span', class_ = 'table-body__logo-text'):\n",
    "    Country.append(i.text)\n",
    "Country.insert (0, 'AUS')\n",
    "\n",
    "Rating = []\n",
    "\n",
    "for i in soup.find_all('td', class_= 'table-body__cell rating'):\n",
    "    Rating.append(i.text)\n",
    "Rating.insert (0,'785')\n",
    "Rating\n",
    "\n",
    "df = pd.DataFrame ({'Rank': Rank, 'Name of the Player': Name, 'Country': Country, 'Rating': Rating})\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b278d78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Name of the Player</th>\n",
       "      <th>Country</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Sophie Ecclestone</td>\n",
       "      <td>ENG</td>\n",
       "      <td>739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Jess Jonassen</td>\n",
       "      <td>AUS</td>\n",
       "      <td>725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Megan Schutt</td>\n",
       "      <td>AUS</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Shabnim Ismail</td>\n",
       "      <td>SA</td>\n",
       "      <td>722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Jhulan Goswami</td>\n",
       "      <td>IND</td>\n",
       "      <td>698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>Hayley Matthews</td>\n",
       "      <td>WI</td>\n",
       "      <td>671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Kate Cross</td>\n",
       "      <td>ENG</td>\n",
       "      <td>657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Ayabonga Khaka</td>\n",
       "      <td>SA</td>\n",
       "      <td>634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Rajeshwari Gayakwad</td>\n",
       "      <td>IND</td>\n",
       "      <td>617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>Marizanne Kapp</td>\n",
       "      <td>SA</td>\n",
       "      <td>598</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Rank   Name of the Player Country Rating\n",
       "0    1    Sophie Ecclestone     ENG    739\n",
       "1    2        Jess Jonassen     AUS    725\n",
       "2    3         Megan Schutt     AUS    722\n",
       "3    4       Shabnim Ismail      SA    722\n",
       "4    5       Jhulan Goswami     IND    698\n",
       "5    6      Hayley Matthews      WI    671\n",
       "6    7           Kate Cross     ENG    657\n",
       "7    8       Ayabonga Khaka      SA    634\n",
       "8    9  Rajeshwari Gayakwad     IND    617\n",
       "9   10       Marizanne Kapp      SA    598"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "# c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "page=requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi/bowling')\n",
    "\n",
    "soup = BeautifulSoup(page.content)\n",
    "\n",
    "Rank = []\n",
    "\n",
    "for i in soup.find_all('span', class_ = 'rankings-table__pos-number'):\n",
    "    Rank.append(i.text)\n",
    "    \n",
    "Rank = [item.strip() for item in Rank if str(item)]\n",
    "Rank.insert (0,'1')\n",
    "\n",
    "Name = []\n",
    "\n",
    "for i in soup.find_all('td',class_='table-body__cell rankings-table__name name'):\n",
    "    Name.append(i.text)\n",
    "Name = [item.strip() for item in Name if str(item)]\n",
    "Name.insert (0,'Sophie Ecclestone')\n",
    "\n",
    "Country = []\n",
    "\n",
    "for i in soup.find_all('span', class_ = 'table-body__logo-text'):\n",
    "    Country.append(i.text)\n",
    "Country.insert (0, 'ENG')\n",
    "\n",
    "Rating = []\n",
    "\n",
    "for i in soup.find_all('td', class_= 'table-body__cell rating'):\n",
    "    Rating.append(i.text)\n",
    "Rating.insert (0,'739')\n",
    "Rating\n",
    "\n",
    "df = pd.DataFrame ({'Rank': Rank, 'Name of the Player': Name, 'Country': Country, 'Rating': Rating})\n",
    "df.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a82dc58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Time</th>\n",
       "      <th>News Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pelosi says her family is 'heartbroken and tra...</td>\n",
       "      <td>2 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/30/nancy-pelosi-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Russia halts participation in an agreement to ...</td>\n",
       "      <td>12 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/russia-halts-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Brooklyn Nets condemn Kyrie Irving for promoti...</td>\n",
       "      <td>13 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/brooklyn-nets-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>These dads drove cross country, raised $156,00...</td>\n",
       "      <td>13 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/dads-road-trip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How 'The 5 Love Languages' stays relevant 30 y...</td>\n",
       "      <td>14 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/how-the-5-love...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I raised 2 successful CEOs and a doctor. Here'...</td>\n",
       "      <td>14 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/i-raised-2-suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Here's a look at some legal and personal ramif...</td>\n",
       "      <td>15 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/here-are-the-l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Issa Rae shares her best career advice and how...</td>\n",
       "      <td>15 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/issa-rae-share...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>These people are making real money in Horizon ...</td>\n",
       "      <td>15 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/metaverse-entr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What 'millionaire tax' plans on California, Ma...</td>\n",
       "      <td>15 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/what-millionai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>A 35-year-old cult classic from John Carpenter...</td>\n",
       "      <td>15 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/john-carpenter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>From Chipotle to Wendy’s, here are 7 places to...</td>\n",
       "      <td>15 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/where-to-score...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Twitter is now owned by Elon Musk — here's a b...</td>\n",
       "      <td>16 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/a-brief-histor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The market is moving in-line with seasonal pat...</td>\n",
       "      <td>17 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/the-market-is-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Stocks reporting earnings in the week ahead th...</td>\n",
       "      <td>17 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/earnings-resul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Goldman Sachs analysts say these are the best ...</td>\n",
       "      <td>17 Hours Ago</td>\n",
       "      <td>https://www.cnbc.com/2022/10/29/goldman-sachs-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Derek and Hannah Jeter sign multiyear deal wit...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/derek-jeter-pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Cramer on Friday named travel as one of five r...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/jim-cramer-say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Cramer's lightning round: Let's stay with Fron...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/cramers-lightn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Cramer's week ahead: There could be 'real sign...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/cramers-week-a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Co-defendant in SEC civil fraud suit against f...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/fake-billionai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>UN details horrifying accounts of rape, tortur...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/russia-ukraine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Stocks shrug off tech's troubles as Street awa...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/stocks-shrugge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Pro Picks: Watch all of Friday's big stock cal...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/pro-picks-watc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GM temporarily suspends advertising on Twitter...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/gm-temporarily...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Big Tech falters on dreary earnings — Meta has...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/big-tech-falte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>93% of daters want a partner who is emotionall...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/how-to-be-emot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Omicron subvariants resistant to key antibody ...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/omicron-subvar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Jeep parent Stellantis offering buyouts to som...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/chrysler-owner...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Musk plans Twitter content moderation council ...</td>\n",
       "      <td>October 28, 2022</td>\n",
       "      <td>https://www.cnbc.com/2022/10/28/musk-plans-twi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Headline              Time  \\\n",
       "0   Pelosi says her family is 'heartbroken and tra...       2 Hours Ago   \n",
       "1   Russia halts participation in an agreement to ...      12 Hours Ago   \n",
       "2   Brooklyn Nets condemn Kyrie Irving for promoti...      13 Hours Ago   \n",
       "3   These dads drove cross country, raised $156,00...      13 Hours Ago   \n",
       "4   How 'The 5 Love Languages' stays relevant 30 y...      14 Hours Ago   \n",
       "5   I raised 2 successful CEOs and a doctor. Here'...      14 Hours Ago   \n",
       "6   Here's a look at some legal and personal ramif...      15 Hours Ago   \n",
       "7   Issa Rae shares her best career advice and how...      15 Hours Ago   \n",
       "8   These people are making real money in Horizon ...      15 Hours Ago   \n",
       "9   What 'millionaire tax' plans on California, Ma...      15 Hours Ago   \n",
       "10  A 35-year-old cult classic from John Carpenter...      15 Hours Ago   \n",
       "11  From Chipotle to Wendy’s, here are 7 places to...      15 Hours Ago   \n",
       "12  Twitter is now owned by Elon Musk — here's a b...      16 Hours Ago   \n",
       "13  The market is moving in-line with seasonal pat...      17 Hours Ago   \n",
       "14  Stocks reporting earnings in the week ahead th...      17 Hours Ago   \n",
       "15  Goldman Sachs analysts say these are the best ...      17 Hours Ago   \n",
       "16  Derek and Hannah Jeter sign multiyear deal wit...  October 28, 2022   \n",
       "17  Cramer on Friday named travel as one of five r...  October 28, 2022   \n",
       "18  Cramer's lightning round: Let's stay with Fron...  October 28, 2022   \n",
       "19  Cramer's week ahead: There could be 'real sign...  October 28, 2022   \n",
       "20  Co-defendant in SEC civil fraud suit against f...  October 28, 2022   \n",
       "21  UN details horrifying accounts of rape, tortur...  October 28, 2022   \n",
       "22  Stocks shrug off tech's troubles as Street awa...  October 28, 2022   \n",
       "23  Pro Picks: Watch all of Friday's big stock cal...  October 28, 2022   \n",
       "24  GM temporarily suspends advertising on Twitter...  October 28, 2022   \n",
       "25  Big Tech falters on dreary earnings — Meta has...  October 28, 2022   \n",
       "26  93% of daters want a partner who is emotionall...  October 28, 2022   \n",
       "27  Omicron subvariants resistant to key antibody ...  October 28, 2022   \n",
       "28  Jeep parent Stellantis offering buyouts to som...  October 28, 2022   \n",
       "29  Musk plans Twitter content moderation council ...  October 28, 2022   \n",
       "\n",
       "                                            News Link  \n",
       "0   https://www.cnbc.com/2022/10/30/nancy-pelosi-s...  \n",
       "1   https://www.cnbc.com/2022/10/29/russia-halts-p...  \n",
       "2   https://www.cnbc.com/2022/10/29/brooklyn-nets-...  \n",
       "3   https://www.cnbc.com/2022/10/29/dads-road-trip...  \n",
       "4   https://www.cnbc.com/2022/10/29/how-the-5-love...  \n",
       "5   https://www.cnbc.com/2022/10/29/i-raised-2-suc...  \n",
       "6   https://www.cnbc.com/2022/10/29/here-are-the-l...  \n",
       "7   https://www.cnbc.com/2022/10/29/issa-rae-share...  \n",
       "8   https://www.cnbc.com/2022/10/29/metaverse-entr...  \n",
       "9   https://www.cnbc.com/2022/10/29/what-millionai...  \n",
       "10  https://www.cnbc.com/2022/10/29/john-carpenter...  \n",
       "11  https://www.cnbc.com/2022/10/29/where-to-score...  \n",
       "12  https://www.cnbc.com/2022/10/29/a-brief-histor...  \n",
       "13  https://www.cnbc.com/2022/10/29/the-market-is-...  \n",
       "14  https://www.cnbc.com/2022/10/29/earnings-resul...  \n",
       "15  https://www.cnbc.com/2022/10/29/goldman-sachs-...  \n",
       "16  https://www.cnbc.com/2022/10/28/derek-jeter-pr...  \n",
       "17  https://www.cnbc.com/2022/10/28/jim-cramer-say...  \n",
       "18  https://www.cnbc.com/2022/10/28/cramers-lightn...  \n",
       "19  https://www.cnbc.com/2022/10/28/cramers-week-a...  \n",
       "20  https://www.cnbc.com/2022/10/28/fake-billionai...  \n",
       "21  https://www.cnbc.com/2022/10/28/russia-ukraine...  \n",
       "22  https://www.cnbc.com/2022/10/28/stocks-shrugge...  \n",
       "23  https://www.cnbc.com/2022/10/28/pro-picks-watc...  \n",
       "24  https://www.cnbc.com/2022/10/28/gm-temporarily...  \n",
       "25  https://www.cnbc.com/2022/10/28/big-tech-falte...  \n",
       "26  https://www.cnbc.com/2022/10/28/how-to-be-emot...  \n",
       "27  https://www.cnbc.com/2022/10/28/omicron-subvar...  \n",
       "28  https://www.cnbc.com/2022/10/28/chrysler-owner...  \n",
       "29  https://www.cnbc.com/2022/10/28/musk-plans-twi...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "# i) Headline\n",
    "# ii) Time\n",
    "# iii) News Link\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import requests as R\n",
    "import pandas as pd\n",
    "\n",
    "page = R.get ('https://www.cnbc.com/world/?region=world')\n",
    "\n",
    "soup = BS (page.content)\n",
    "\n",
    "Headlines  = []\n",
    "    \n",
    "for i in soup.find_all('a', class_= 'LatestNews-headline'):\n",
    "    Headlines.append(i.text)\n",
    "\n",
    "Time = []\n",
    "\n",
    "for i in soup.find_all('span', class_ = 'LatestNews-wrapper'):\n",
    "    Time.append(i.text)\n",
    "\n",
    "Link = []\n",
    "\n",
    "for i in soup.find_all('a', class_ = 'LatestNews-headline'):\n",
    "    Link.append(i.get('href'))\n",
    "Link\n",
    "\n",
    "df = pd.DataFrame({'Headline': Headlines, 'Time': Time, 'News Link': Link})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00fc0d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Paper Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Paper URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Reward is enough</td>\n",
       "      <td>Silver, David, Singh, Satinder, Precup, Doina,...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Making sense of raw input</td>\n",
       "      <td>Evans, Richard, Bošnjak, Matko and 5 more</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Law and logic: A review from an argumentation ...</td>\n",
       "      <td>Prakken, Henry, Sartor, Giovanni</td>\n",
       "      <td>October 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Creativity and artificial intelligence</td>\n",
       "      <td>Boden, Margaret A.</td>\n",
       "      <td>August 1998</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Artificial cognition for social human–robot in...</td>\n",
       "      <td>Lemaignan, Séverin, Warnier, Mathieu and 3 more</td>\n",
       "      <td>June 2017</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Explanation in artificial intelligence: Insigh...</td>\n",
       "      <td>Miller, Tim</td>\n",
       "      <td>February 2019</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Making sense of sensory input</td>\n",
       "      <td>Evans, Richard, Hernández-Orallo, José and 3 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Conflict-based search for optimal multi-agent ...</td>\n",
       "      <td>Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...</td>\n",
       "      <td>February 2015</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Between MDPs and semi-MDPs: A framework for te...</td>\n",
       "      <td>Sutton, Richard S., Precup, Doina, Singh, Sati...</td>\n",
       "      <td>August 1999</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Hanabi challenge: A new frontier for AI re...</td>\n",
       "      <td>Bard, Nolan, Foerster, Jakob N. and 13 more</td>\n",
       "      <td>March 2020</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Evaluating XAI: A comparison of rule-based and...</td>\n",
       "      <td>van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...</td>\n",
       "      <td>February 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Argumentation in artificial intelligence</td>\n",
       "      <td>Bench-Capon, T.J.M., Dunne, Paul E.</td>\n",
       "      <td>October 2007</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Algorithms for computing strategies in two-pla...</td>\n",
       "      <td>Bošanský, Branislav, Lisý, Viliam and 3 more</td>\n",
       "      <td>August 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Multiple object tracking: A literature review</td>\n",
       "      <td>Luo, Wenhan, Xing, Junliang and 4 more</td>\n",
       "      <td>April 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Selection of relevant features and examples in...</td>\n",
       "      <td>Blum, Avrim L., Langley, Pat</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>A survey of inverse reinforcement learning: Ch...</td>\n",
       "      <td>Arora, Saurabh, Doshi, Prashant</td>\n",
       "      <td>August 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Explaining individual predictions when feature...</td>\n",
       "      <td>Aas, Kjersti, Jullum, Martin, Løland, Anders</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>A review of possible effects of cognitive bias...</td>\n",
       "      <td>Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...</td>\n",
       "      <td>June 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Integrating social power into the decision-mak...</td>\n",
       "      <td>Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.</td>\n",
       "      <td>December 2016</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>“That's (not) the output I expected!” On the r...</td>\n",
       "      <td>Riveiro, Maria, Thill, Serge</td>\n",
       "      <td>September 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Explaining black-box classifiers using post-ho...</td>\n",
       "      <td>Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...</td>\n",
       "      <td>May 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Algorithm runtime prediction: Methods &amp; evalua...</td>\n",
       "      <td>Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...</td>\n",
       "      <td>January 2014</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Wrappers for feature subset selection</td>\n",
       "      <td>Kohavi, Ron, John, George H.</td>\n",
       "      <td>December 1997</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Commonsense visual sensemaking for autonomous ...</td>\n",
       "      <td>Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...</td>\n",
       "      <td>October 2021</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Quantum computation, quantum theory and AI</td>\n",
       "      <td>Ying, Mingsheng</td>\n",
       "      <td>February 2010</td>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Paper Title  \\\n",
       "0                                    Reward is enough   \n",
       "1                           Making sense of raw input   \n",
       "2   Law and logic: A review from an argumentation ...   \n",
       "3              Creativity and artificial intelligence   \n",
       "4   Artificial cognition for social human–robot in...   \n",
       "5   Explanation in artificial intelligence: Insigh...   \n",
       "6                       Making sense of sensory input   \n",
       "7   Conflict-based search for optimal multi-agent ...   \n",
       "8   Between MDPs and semi-MDPs: A framework for te...   \n",
       "9   The Hanabi challenge: A new frontier for AI re...   \n",
       "10  Evaluating XAI: A comparison of rule-based and...   \n",
       "11           Argumentation in artificial intelligence   \n",
       "12  Algorithms for computing strategies in two-pla...   \n",
       "13      Multiple object tracking: A literature review   \n",
       "14  Selection of relevant features and examples in...   \n",
       "15  A survey of inverse reinforcement learning: Ch...   \n",
       "16  Explaining individual predictions when feature...   \n",
       "17  A review of possible effects of cognitive bias...   \n",
       "18  Integrating social power into the decision-mak...   \n",
       "19  “That's (not) the output I expected!” On the r...   \n",
       "20  Explaining black-box classifiers using post-ho...   \n",
       "21  Algorithm runtime prediction: Methods & evalua...   \n",
       "22              Wrappers for feature subset selection   \n",
       "23  Commonsense visual sensemaking for autonomous ...   \n",
       "24         Quantum computation, quantum theory and AI   \n",
       "\n",
       "                                              Authors  Published Date  \\\n",
       "0   Silver, David, Singh, Satinder, Precup, Doina,...    October 2021   \n",
       "1           Evans, Richard, Bošnjak, Matko and 5 more    October 2021   \n",
       "2                   Prakken, Henry, Sartor, Giovanni     October 2015   \n",
       "3                                 Boden, Margaret A.      August 1998   \n",
       "4     Lemaignan, Séverin, Warnier, Mathieu and 3 more       June 2017   \n",
       "5                                        Miller, Tim    February 2019   \n",
       "6   Evans, Richard, Hernández-Orallo, José and 3 more      April 2021   \n",
       "7   Sharon, Guni, Stern, Roni, Felner, Ariel, Stur...   February 2015   \n",
       "8   Sutton, Richard S., Precup, Doina, Singh, Sati...     August 1999   \n",
       "9         Bard, Nolan, Foerster, Jakob N. and 13 more      March 2020   \n",
       "10  van der Waa, Jasper, Nieuwburg, Elisabeth, Cre...   February 2021   \n",
       "11               Bench-Capon, T.J.M., Dunne, Paul E.     October 2007   \n",
       "12       Bošanský, Branislav, Lisý, Viliam and 3 more     August 2016   \n",
       "13             Luo, Wenhan, Xing, Junliang and 4 more      April 2021   \n",
       "14                      Blum, Avrim L., Langley, Pat    December 1997   \n",
       "15                   Arora, Saurabh, Doshi, Prashant      August 2021   \n",
       "16      Aas, Kjersti, Jullum, Martin, Løland, Anders   September 2021   \n",
       "17  Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Joha...       June 2021   \n",
       "18    Pereira, Gonçalo, Prada, Rui, Santos, Pedro A.    December 2016   \n",
       "19                      Riveiro, Maria, Thill, Serge   September 2021   \n",
       "20  Kenny, Eoin M., Ford, Courtney, Quinn, Molly, ...        May 2021   \n",
       "21  Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyto...    January 2014   \n",
       "22                      Kohavi, Ron, John, George H.    December 1997   \n",
       "23  Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srik...    October 2021   \n",
       "24                                   Ying, Mingsheng    February 2010   \n",
       "\n",
       "                                            Paper URL  \n",
       "0   https://www.sciencedirect.com/science/article/...  \n",
       "1   https://www.sciencedirect.com/science/article/...  \n",
       "2   https://www.sciencedirect.com/science/article/...  \n",
       "3   https://www.sciencedirect.com/science/article/...  \n",
       "4   https://www.sciencedirect.com/science/article/...  \n",
       "5   https://www.sciencedirect.com/science/article/...  \n",
       "6   https://www.sciencedirect.com/science/article/...  \n",
       "7   https://www.sciencedirect.com/science/article/...  \n",
       "8   https://www.sciencedirect.com/science/article/...  \n",
       "9   https://www.sciencedirect.com/science/article/...  \n",
       "10  https://www.sciencedirect.com/science/article/...  \n",
       "11  https://www.sciencedirect.com/science/article/...  \n",
       "12  https://www.sciencedirect.com/science/article/...  \n",
       "13  https://www.sciencedirect.com/science/article/...  \n",
       "14  https://www.sciencedirect.com/science/article/...  \n",
       "15  https://www.sciencedirect.com/science/article/...  \n",
       "16  https://www.sciencedirect.com/science/article/...  \n",
       "17  https://www.sciencedirect.com/science/article/...  \n",
       "18  https://www.sciencedirect.com/science/article/...  \n",
       "19  https://www.sciencedirect.com/science/article/...  \n",
       "20  https://www.sciencedirect.com/science/article/...  \n",
       "21  https://www.sciencedirect.com/science/article/...  \n",
       "22  https://www.sciencedirect.com/science/article/...  \n",
       "23  https://www.sciencedirect.com/science/article/...  \n",
       "24  https://www.sciencedirect.com/science/article/...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days. \n",
    "# https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "# Scrape below mentioned details :\n",
    "# i) Paper Title \n",
    "# ii) Authors\n",
    "# iii) Published Date \n",
    "# iv) Paper URL\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import requests as R\n",
    "import pandas as pd\n",
    "\n",
    "page = R.get ('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "\n",
    "\n",
    "soup = BS (page.content)\n",
    "Title = []\n",
    "\n",
    "for i in soup.find_all('h2',class_='sc-1qrq3sd-1 MKjKb sc-1nmom32-0 sc-1nmom32-1 hqhUYH ebTA-dR'):\n",
    "    Title.append(i.text)\n",
    "    \n",
    "Authors= []\n",
    "\n",
    "for i in soup.find_all('span', class_ = 'sc-1w3fpd7-0 pgLAT'):\n",
    "    Authors.append(i.text)\n",
    "    \n",
    "Date = []\n",
    "\n",
    "for i in soup.find_all('span', class_= 'sc-1thf9ly-2 bKddwo'):\n",
    "    Date.append(i.text)\n",
    "\n",
    "URL = []\n",
    "\n",
    "for i in soup.find_all('a', class_ = 'sc-5smygv-0 nrDZj'):\n",
    "    URL.append(i.get('href'))\n",
    "    \n",
    "df = pd.DataFrame ({'Paper Title': Title, 'Authors': Authors, 'Published Date': Date, 'Paper URL':URL})\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba9305fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name of Restaurant</th>\n",
       "      <th>Cuisine</th>\n",
       "      <th>Location</th>\n",
       "      <th>Ratings</th>\n",
       "      <th>URL of Image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Connaught Place, Central Delhi</td>\n",
       "      <td>4.1</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jungle Jamboree</td>\n",
       "      <td>North Indian, Asian, Italian</td>\n",
       "      <td>3CS Mall,Lajpat Nagar - 3, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Castle Barbeque</td>\n",
       "      <td>Chinese, North Indian</td>\n",
       "      <td>Pacific Mall,Tagore Garden, West Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cafe Knosh</td>\n",
       "      <td>Italian, Continental</td>\n",
       "      <td>The Leela Ambience Convention Hotel,Shahdara, ...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Barbeque Company</td>\n",
       "      <td>North Indian, Chinese</td>\n",
       "      <td>Gardens Galleria,Sector 38A, Noida</td>\n",
       "      <td>4</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>India Grill</td>\n",
       "      <td>North Indian, Italian</td>\n",
       "      <td>Hilton Garden Inn,Saket, South Delhi</td>\n",
       "      <td>3.9</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi Barbeque</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Taurus Sarovar Portico,Mahipalpur, South Delhi</td>\n",
       "      <td>3.6</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The Monarch - Bar Be Que Village</td>\n",
       "      <td>North Indian</td>\n",
       "      <td>Indirapuram Habitat Centre,Indirapuram, Ghaziabad</td>\n",
       "      <td>3.8</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Indian Grill Room</td>\n",
       "      <td>North Indian, Mughlai</td>\n",
       "      <td>Suncity Business Tower,Golf Course Road, Gurgaon</td>\n",
       "      <td>4.3</td>\n",
       "      <td>https://im1.dineout.co.in/images/uploads/resta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Name of Restaurant                        Cuisine  \\\n",
       "0                   Castle Barbeque          Chinese, North Indian   \n",
       "1                   Jungle Jamboree   North Indian, Asian, Italian   \n",
       "2                   Castle Barbeque          Chinese, North Indian   \n",
       "3                        Cafe Knosh           Italian, Continental   \n",
       "4              The Barbeque Company          North Indian, Chinese   \n",
       "5                       India Grill          North Indian, Italian   \n",
       "6                    Delhi Barbeque                   North Indian   \n",
       "7  The Monarch - Bar Be Que Village                   North Indian   \n",
       "8                 Indian Grill Room          North Indian, Mughlai   \n",
       "\n",
       "                                            Location Ratings  \\\n",
       "0                     Connaught Place, Central Delhi     4.1   \n",
       "1             3CS Mall,Lajpat Nagar - 3, South Delhi     3.9   \n",
       "2             Pacific Mall,Tagore Garden, West Delhi     3.9   \n",
       "3  The Leela Ambience Convention Hotel,Shahdara, ...     4.3   \n",
       "4                 Gardens Galleria,Sector 38A, Noida       4   \n",
       "5               Hilton Garden Inn,Saket, South Delhi     3.9   \n",
       "6     Taurus Sarovar Portico,Mahipalpur, South Delhi     3.6   \n",
       "7  Indirapuram Habitat Centre,Indirapuram, Ghaziabad     3.8   \n",
       "8   Suncity Business Tower,Golf Course Road, Gurgaon     4.3   \n",
       "\n",
       "                                        URL of Image  \n",
       "0  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "1  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "2  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "3  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "4  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "5  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "6  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "7  https://im1.dineout.co.in/images/uploads/resta...  \n",
       "8  https://im1.dineout.co.in/images/uploads/resta...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "# i) Restaurant name\n",
    "# ii) Cuisine\n",
    "# iii) Location \n",
    "# iv) Ratings\n",
    "# v) Image URL\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import requests as R\n",
    "import pandas as pd\n",
    "\n",
    "page = R.get ('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "soup = BS (page.content)\n",
    "\n",
    "Name = []\n",
    "\n",
    "for i in soup.find_all('a', class_= 'restnt-name ellipsis'):\n",
    "    Name.append(i.text)\n",
    "\n",
    "Cuisine = []\n",
    "\n",
    "for i in soup.find_all('span', class_='double-line-ellipsis'):\n",
    "    Cuisine.append(i.text.split('|')[1])\n",
    "    \n",
    "Location = []\n",
    "\n",
    "for i in soup.find_all('div', class_= 'restnt-loc ellipsis'):\n",
    "    Location.append(i.text)\n",
    "    \n",
    "\n",
    "Ratings = []\n",
    "\n",
    "for i in soup.find_all('div', class_ = 'restnt-rating rating-4'):\n",
    "    Ratings.append(i.text)\n",
    "    \n",
    "    \n",
    "Image = []\n",
    "\n",
    "for i in soup.find_all('img', class_='no-img'):\n",
    "    Image.append(i.get('data-src'))\n",
    "    \n",
    "df = pd.DataFrame ({'Name of Restaurant': Name, 'Cuisine': Cuisine, 'Location': Location, 'Ratings':Ratings, 'URL of Image':Image })\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f41eec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank</th>\n",
       "      <th>Publication</th>\n",
       "      <th>h5_index</th>\n",
       "      <th>h5_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.</td>\n",
       "      <td>Nature</td>\n",
       "      <td>444</td>\n",
       "      <td>667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.</td>\n",
       "      <td>The New England Journal of Medicine</td>\n",
       "      <td>432</td>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.</td>\n",
       "      <td>Science</td>\n",
       "      <td>401</td>\n",
       "      <td>614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.</td>\n",
       "      <td>IEEE/CVF Conference on Computer Vision and Pat...</td>\n",
       "      <td>389</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.</td>\n",
       "      <td>The Lancet</td>\n",
       "      <td>354</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>96.</td>\n",
       "      <td>Journal of Business Research</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97.</td>\n",
       "      <td>Molecular Cancer</td>\n",
       "      <td>145</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98.</td>\n",
       "      <td>Sensors</td>\n",
       "      <td>145</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>99.</td>\n",
       "      <td>Nature Climate Change</td>\n",
       "      <td>144</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>100.</td>\n",
       "      <td>IEEE Internet of Things Journal</td>\n",
       "      <td>144</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Rank                                        Publication h5_index h5_median\n",
       "0     1.                                             Nature      444       667\n",
       "1     2.                The New England Journal of Medicine      432       780\n",
       "2     3.                                            Science      401       614\n",
       "3     4.  IEEE/CVF Conference on Computer Vision and Pat...      389       627\n",
       "4     5.                                         The Lancet      354       635\n",
       "..   ...                                                ...      ...       ...\n",
       "95   96.                       Journal of Business Research      145       233\n",
       "96   97.                                   Molecular Cancer      145       209\n",
       "97   98.                                            Sensors      145       201\n",
       "98   99.                              Nature Climate Change      144       228\n",
       "99  100.                    IEEE Internet of Things Journal      144       212\n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10) Write a python program to scrape the details of top publications from Google Scholar from \n",
    "#https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "# i) Rank \n",
    "# ii) Publication\n",
    "# iii) h5-index\n",
    "# iv) h5-median\n",
    " \n",
    "\n",
    "from bs4 import BeautifulSoup as BS\n",
    "import requests as R\n",
    "import pandas as pd\n",
    "\n",
    "page = R.get ('https://scholar.google.com/citations?view_op=top_venues&hl=en')\n",
    "\n",
    "soup = BS (page.content)\n",
    "\n",
    "Rank = []\n",
    "\n",
    "for i in soup.find_all('td', class_= 'gsc_mvt_p'):\n",
    "    Rank.append(i.text)\n",
    "\n",
    "    \n",
    "Publication = []\n",
    "\n",
    "for i in soup.find_all('td', class_ = 'gsc_mvt_t'):\n",
    "    Publication.append(i.text)\n",
    "\n",
    "h5_index = []\n",
    "\n",
    "for i in soup.find_all('td', class_ = 'gsc_mvt_n'):\n",
    "    h5_index.append(i.text)\n",
    "del h5_index [1::2]\n",
    "\n",
    "h5_median = []\n",
    "\n",
    "for i in soup.find_all('td', class_ = 'gsc_mvt_n'):\n",
    "    h5_median.append(i.text)\n",
    "del h5_median [0::2]\n",
    "\n",
    "df = pd.DataFrame ({'Rank': Rank, 'Publication':Publication, 'h5_index': h5_index, 'h5_median':h5_median})\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
